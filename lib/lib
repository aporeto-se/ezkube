#!/bin/bash
##########################################################################################

##########################################################################################

err() { echo "$@" 1>&2; }

function check-settings() {

  cd $(dirname $0)
  [ -f settings ] && . settings || { err "file settings not found"; return 2; }
  local fatal=0
  [[ $CLOUD_PROVIDER ]] || { err "Missing setting CLOUD_PROVIDER"; fatal=1; }
  [[ $CLUSTER_NAME ]] || { err "Missing setting CLUSTER_NAME"; fatal=1; }
  [[ $DNS_DOMAIN ]] || { err "Missing setting DNS_DOMAIN"; fatal=1; }
  [[ $PARENT_NAMESPACE ]] || { err "Missing setting PARENT_NAMESPACE"; fatal=1; }
  [[ $APOCTL_API ]] || { err "Missing setting APOCTL_API"; fatal=1; }

  _aws() {
    local fatal=0
    [[ $AWS_KUBE_REGION ]] || { err "Missing setting AWS_KUBE_REGION"; fatal=1; }
    [[ $AWS_KUBE_ZONE ]] || { err "Missing setting AWS_KUBE_ZONE"; fatal=1; }
    [[ $AWS_KUBE_VERSION ]] || { err "Missing setting AWS_KUBE_VERSION"; fatal=1; }
    [[ $AWS_KUBE_MASTER_SIZE ]] || { err "Missing setting AWS_KUBE_MASTER_SIZE"; fatal=1; }
    [[ $AWS_KUBE_NODE_SIZE ]] || { err "Missing setting AWS_KUBE_NODE_SIZE"; fatal=1; }
    [[ $AWS_KUBE_IMAGE ]] || { err "Missing setting AWS_KUBE_IMAGE"; fatal=1; }
    [[ $AWS_KUBE_NODE_COUNT ]] || { err "Missing setting AWS_KUBE_NODE_COUNT"; fatal=1; }
    [[ $AWS_KUBE_CNI ]] || { err "Missing setting AWS_KUBE_CNI"; fatal=1; }
    [[ $AWS_ACCOUNT ]] || { err "Missing setting AWS_ACCOUNT"; fatal=1; }
    [[ $AWS_DOMAIN_ID ]] || { err "Missing setting AWS_DOMAIN_ID"; fatal=1; }
    [ $fatal -eq 1 ] && return 2
    err "config is valid"
    return 0
  }

  [[ "$CLOUD_PROVIDER" == "AWS" ]] && { _aws; return $?; }
  err "CLOUD_PROVIDER $CLOUD_PROVIDER is not known"
  return 2
}

function check-system() {
  cd $(dirname $0)
  [ -f settings ] && . settings || { err "file settings not found"; return 2; }
  local fatal=0
  which kubectl > /dev/null 2>&1 || { err "kubectl not in path"; fatal=1; }
  which aws > /dev/null 2>&1 || { err "aws not in path"; fatal=1; }
  which kops > /dev/null 2>&1 || { err "kops not in path"; fatal=1; }
  which ssh-keygen > /dev/null 2>&1 || { err "kops not in path"; fatal=1; }
  which openssl > /dev/null 2>&1 || { err "openssl not in path"; fatal=1; }
  [ $fatal -eq 1 ] && { err "system is not ready"; return 2; }
  err "system is ready"
}

function create-cluster() {
  cd $(dirname $0)
  [ -f settings ] && . settings || { err "file settings not found"; return 2; }
  local fatal=0

  check-system || { fatal=1; }
  check-config || { fatal=1; }
  [ $fatal -eq 1 ] && { err "Unable to continue. See above."; return 2; }

  SSHKEY=${HOME}/.ssh/aporeto_lab
  FQDN=${CLUSTER_NAME}.${DNS_DOMAIN}

  [ -f $SSHKEY ] || {
    err "Key $SSHKEY is not present; copying from S3"
    aws s3 cp s3://labkeys.aporeto.io/ssh/key $SSHKEY || return $?
    chmod 400 ${HOME}/.ssh/aporeto_lab || return $?
    ssh-keygen -y -f $SSHKEY > ${SSHKEY}.pub || return $?
  }

  [ -f ${HOME}/.ssh/config ] || {
    err "Warning: you did not have a ~/.ssh/config file so I will create one for you"
    echo "IdentityFile ~/.ssh/id_ecdsa" >> ${HOME}/.ssh/config
    echo "IdentityFile ~/.ssh/id_rsa" >> ${HOME}/.ssh/config
    err "You might want to check it"
  }

  [[  $(cat ~/.ssh/config | grep aporeto_lab) ]] || {
    echo "IdentityFile ~/.ssh/aporeto_lab" >> ${HOME}/.ssh/config
  }

  _aws() {
    local fatal=0
    [[ $AWS_KUBE_REGION ]] || { err "Missing setting AWS_KUBE_REGION"; fatal=1; }
    [[ $AWS_KUBE_ZONE ]] || { err "Missing setting AWS_KUBE_ZONE"; fatal=1; }
    [ $fatal -eq 1 ] && return 2
    bucket="s3://k8s.${FQDN}"
    zones="${AWS_KUBE_REGION}${AWS_KUBE_ZONE}"
    kops="kops create cluster"
    kops+=" --name $FQDN"
    kops+=" --zones=$zones"
    kops+=" --master-size=$AWS_KUBE_MASTER_SIZE"
    kops+=" --node-size=$AWS_KUBE_NODE_SIZE"
    kops+=" --node-count=$AWS_KUBE_NODE_COUNT"
    kops+=" --ssh-public-key=${SSHKEY}.pub"
    kops+=" --state=${bucket}/state"
    [[ $AWS_KUBE_VERSION ]] && kops+=" --kubernetes-version $AWS_KUBE_VERSION"
    [[ $AWS_KUBE_IMAGE ]] && kops+=" --image $AWS_KUBE_IMAGE"
    [[ $AWS_KUBE_CNI ]] && kops+=" --networking=$AWS_KUBE_CNI"
    kops+=" --yes"
    export KUBECONFIG=$PWD/kube.config
    set -x
    aws s3 mb $bucket || return $?
    $kops || return $?
    kops export kubecfg $FQDN --state=${bucket}/state || return $?
  }

  [[ "$CLOUD_PROVIDER" == "AWS" ]] && { _aws; return $?; }
}

function delete-cluster() {
  cd $(dirname $0)
  [ -f settings ] && . settings || { err "file settings not found"; return 2; }
  local fatal=0
  check-system || { fatal=1; }
  check-config || { fatal=1; }
  [ $fatal -eq 1 ] && { err "Unable to continue. See above."; return 2; }

  FQDN=${CLUSTER_NAME}.${DNS_DOMAIN}

  _aws() {
    local fatal=0
    local bucket="s3://k8s.${FQDN}"
    set -x
    kops delete cluster --name $FQDN --state=${bucket}/state --yes
    aws s3 rb $bucket --force
  }
  [[ "$CLOUD_PROVIDER" == "AWS" ]] && { _aws; return $?; }

  err "CLOUD_PROVIDER $CLOUD_PROVIDER not known"
  return 2
}

function deploy-dashboard() {
  cd $(dirname $0)
  [ -f settings ] && . settings || { err "file settings not found"; return 2; }
  local fatal=0
  ./check-system || { fatal=1; }
  ./check-config || { fatal=1; }
  [ $fatal -eq 1 ] && { err "Unable to continue. See above."; return 2; }

  FQDN=${CLUSTER_NAME}.${DNS_DOMAIN}

  local bucket="s3://k8s.${FQDN}"
  export KUBECONFIG=${PWD}/kube.config
  [ -f kube.config ] || { kops export kubecfg $FQDN --state=${bucket}/state; }
  kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta1/aio/deploy/recommended.yaml
  kubectl apply -f config/dashboard-adminuser.yaml
}

function get-nodes() {
  cd $(dirname $0)
  [ -f settings ] && . settings || { err "file settings not found"; return 2; }
  local fatal=0
  check-system || { fatal=1; }
  check-config || { fatal=1; }
  [ $fatal -eq 1 ] && { err "Unable to continue. See above."; return 2; }

  FQDN=${CLUSTER_NAME}.${DNS_DOMAIN}

  _aws() {
    aws ec2 --region $AWS_KUBE_REGION describe-instances --filters --filters \
      "Name=iam-instance-profile.arn,Values=*$FQDN" \
      --query "Reservations[*].Instances[*].PublicDnsName" --output text
  }

  [[ "$CLOUD_PROVIDER" == "AWS" ]] && { _aws; return $?; }
  err "CLOUD_PROVIDER $CLOUD_PROVIDER not known"
  return 2
}

function load-kubecfg() {
  cd $(dirname $0)
  [ -f settings ] && . settings || { err "file settings not found"; return 2; }
  local fatal=0
  check-system || { fatal=1; }
  check-config || { fatal=1; }
  [ $fatal -eq 1 ] && { err "Unable to continue. See above."; return 2; }

  FQDN=${CLUSTER_NAME}.${DNS_DOMAIN}

  [ -f kube.config ] && { err "kube.config exist"; return 0; }

  _aws() {
    local fatal=0
    local bucket="s3://k8s.${FQDN}"
    export KUBECONFIG=${PWD}/kube.config
    kops export kubecfg $FQDN --state=${bucket}/state
  }
  [[ "$CLOUD_PROVIDER" == "AWS" ]] && { _aws; return $?; }

  err "CLOUD_PROVIDER $CLOUD_PROVIDER not known"
  return 2
}

function patch-aws-cni() {
  cd $(dirname $0)
  [ -f settings ] && . settings || { err "file settings not found"; return 2; }
  local fatal=0
  check-system || { fatal=1; }
  check-config || { fatal=1; }
  [ $fatal -eq 1 ] && { err "Unable to continue. See above."; return 2; }

  [[ "$CLOUD_PROVIDER" == "AWS" ]] || { err "Cloud provider must be AWS"; return 2; }
  [[ "$AWS_KUBE_CNI" == "amazon-vpc-routed-eni" ]] || { err "CNI must be amazon-vpc-routed-eni"; return 2;  }

  FQDN=${CLUSTER_NAME}.${DNS_DOMAIN}
  local bucket="s3://k8s.${FQDN}"
  [ -f kube.config ] || {
    export KUBECONFIG=${PWD}/kube.config
    kops export kubecfg $FQDN --state=${bucket}/state
  }
  kubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/release-1.5/config/v1.5/aws-k8s-cni.yaml
}

function setup-aporeto() {
  cd $(dirname $0)
  [ -f settings ] && . settings || { err "file settings not found"; return 2; }
  local fatal=0
  check-system || { fatal=1; }
  check-config || { fatal=1; }
  [ $fatal -eq 1 ] && { err "Unable to continue. See above."; return 2; }

  FQDN=${CLUSTER_NAME}.${DNS_DOMAIN}
  local bucket="s3://k8s.${FQDN}"
  NAMESPACE=${PARENT_NAMESPACE}/${CLUSTER_NAME}

  function _aws() {
    local fatal=0
    [[ $AWS_ACCOUNT ]] || { err "Missing setting AWS_ACCOUNT"; fatal=1; }
    [ $fatal -eq 1 ] && return 2
  
    CONFIG=.config
    mkdir -p $CONFIG
    _aws_s3_policy > $CONFIG/s3_policy.json
    _aws_ec2_policy > $CONFIG/ec2_policy.json
    _enforcerd > $CONFIG/enforcerd.conf
    _aws_auth_masters > $CONFIG/aws_auth_masters.json
    _aws_auth_nodes > $CONFIG/aws_auth_nodes.json
    _cert > $CONFIG/cert.yaml
  
    cat config/aporeto-policy.yaml | sed "s/:::CLUSTER_NAME:::/$CLUSTER_NAME/g" > $CONFIG/aporeto-policy.yaml
  
    openssl ecparam -name prime256v1 -genkey -noout -out ${CONFIG}/key.pem
    openssl req -new -key ${CONFIG}/key.pem -subj "/CN=system:serviceaccount:aporeto:enforcerd" -out ${CONFIG}/key.csr
  
    set -x
  
    kubectl create namespace aporeto
    apoctl api create namespace --namespace $PARENT_NAMESPACE --api $APOCTL_API --data "{\"name\": \"$CLUSTER_NAME\"}"
    apoctl api import --namespace $NAMESPACE --api $APOCTL_API --file ${CONFIG}/aporeto-policy.yaml
    apoctl api create apiauthorizationpolicy --namespace $NAMESPACE -f ${CONFIG}/aws_auth_masters.json
    apoctl api create apiauthorizationpolicy --namespace $NAMESPACE -f ${CONFIG}/aws_auth_nodes.json
    apoctl appcred create aporeto-operator --type k8s --role "@auth:role=aporeto-operator" --namespace $NAMESPACE > appcred.yaml
    kubectl apply -f appcred.yaml -n aporeto
    aws iam put-role-policy --role-name masters.${FQDN} --policy-name AporetoS3 --policy-document file://${CONFIG}/s3_policy.json
    aws iam put-role-policy --role-name masters.${FQDN} --policy-name AporetoEC2 --policy-document file://${CONFIG}/ec2_policy.json
    aws iam put-role-policy --role-name nodes.${FQDN} --policy-name AporetoS3 --policy-document file://${CONFIG}/s3_policy.json
    aws iam put-role-policy --role-name nodes.${FQDN} --policy-name AporetoEC2 --policy-document file://${CONFIG}/ec2_policy.json
    kubectl apply -f config/k8s_tiller_account.yaml
    kubectl apply -f config/k8s_enforcerd_account.yaml
    sleep 4
    helm init --service-account tiller --upgrade
    sleep 4
    helm repo add aporeto https://charts.aporeto.com/clients
    sleep 4
    helm install aporeto/aporeto-crds --name aporeto-crds
    sleep 4
    helm install aporeto/aporeto-operator --name aporeto-operator --namespace aporeto
    kubectl create -f $CONFIG/cert.yaml
    kubectl certificate approve enforcerd-client
    kubectl get csr enforcerd-client -o jsonpath='{.status.certificate}' | base64 --decode > ${CONFIG}/key.crt
    kubectl get secret $(kubectl get serviceaccounts default -o=jsonpath='{.secrets[0].name}') -o=jsonpath='{.data.ca\.crt}' | base64 --decode > ${CONFIG}/ca.crt
    kubectl --kubeconfig=$CONFIG}/enforcerd.kubeconfig config set-cluster enforcerd --embed-certs=true --server=https://api.internal.$FQDN --certificate-authority=${CONFIG}/ca.crt
    kubectl --kubeconfig=${CONFIG}/enforcerd.kubeconfig config set-credentials enforcerd --embed-certs=true \
      --username=system:serviceaccount:aporeto:enforcerd --client-key=${CONFIG}/key.pem --client-certificate=${CONFIG}/key.crt
    kubectl --kubeconfig=${CONFIG}/enforcerd.kubeconfig config set-context enforcerd --cluster=enforcerd --user=enforcerd
    kubectl --kubeconfig=${CONFIG}/enforcerd.kubeconfig config use-context enforcerd
    aws s3 cp ${CONFIG}/enforcerd.conf ${bucket}/aporeto/enforcerd.conf
    aws s3 cp ${CONFIG}/enforcerd.kubeconfig ${bucket}/aporeto/enforcerd.kubeconfig
  }

  [[ "$CLOUD_PROVIDER" == "AWS" ]] && { _aws; return $?; }
  err "CLOUD_PROVIDER $CLOUD_PROVIDER not known"
    return 2
}

##########################################################################################

_enforcerd()
{
cat << EOF
ENFORCERD_NAMESPACE=$NAMESPACE
ENFORCERD_ENABLE_KUBERNETES=true
ENFORCERD_COMPRESSED_TAGS=true
ENFORCERD_PU_METADATA_EXTRACTOR=docker
ENFORCERD_KUBERNETES_METADATA_EXTRACTOR=podAtomic
ENFORCERD_KUBENODE=\$(hostname)
ENFORCERD_KUBECONFIG=/etc/enforcerd.kubeconfig
EOF
}

_aws_s3_policy()
{
cat <<'EOF'
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": [
        "s3:ListBucketByTags",
        "s3:GetLifecycleConfiguration",
        "s3:GetBucketTagging",
        "s3:GetInventoryConfiguration",
        "s3:GetObjectVersionTagging",
        "s3:ListBucketVersions",
        "s3:GetBucketLogging",
        "s3:ListBucket",
        "s3:GetAccelerateConfiguration",
        "s3:GetBucketPolicy",
        "s3:GetObjectVersionTorrent",
        "s3:GetObjectAcl",
        "s3:GetEncryptionConfiguration",
        "s3:GetBucketRequestPayment",
        "s3:GetObjectVersionAcl",
        "s3:GetObjectTagging",
        "s3:GetMetricsConfiguration",
        "s3:HeadBucket",
        "s3:GetBucketPublicAccessBlock",
        "s3:GetBucketPolicyStatus",
        "s3:ListBucketMultipartUploads",
        "s3:GetBucketWebsite",
        "s3:ListJobs",
        "s3:GetBucketVersioning",
        "s3:GetBucketAcl",
        "s3:GetBucketNotification",
        "s3:GetReplicationConfiguration",
        "s3:ListMultipartUploadParts",
        "s3:GetObject",
        "s3:GetObjectTorrent",
        "s3:GetAccountPublicAccessBlock",
        "s3:ListAllMyBuckets",
        "s3:DescribeJob",
        "s3:GetBucketCORS",
        "s3:GetAnalyticsConfiguration",
        "s3:GetObjectVersionForReplication",
        "s3:GetBucketLocation",
        "s3:GetObjectVersion"
      ],
EOF
echo "      \"Resource\": \"arn:aws:s3:::k8s.$FQDN/aporeto/*\","
cat <<'EOF'
      "Effect": "Allow",
      "Sid": "AporetoEnforcerSystemD"
    }
  ]
}
EOF
}

_aws_ec2_policy()
{
cat <<'EOF'
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "GetClusterName",
      "Effect": "Allow",
      "Action": [
        "ec2:DescribeTags"
      ],
      "Resource": "*"
    }
  ]
}
EOF
}

_aws_auth_masters()
{
cat <<EOF
{
  "authorizedNamespace": "$NAMESPACE",
  "allowsGet": false,
  "allowsPost": false,
  "allowsPut": false,
  "allowsDelete": false,
  "allowsPatch": false,
  "allowsHead": true,
  "propagate": true,
  "name": "Cluster Masters",
  "subject": [
    [
      "@auth:account=$AWS_ACCOUNT",
      "@auth:realm=awssecuritytoken",
      "@auth:rolename=masters.$FQDN"
    ]
  ],
  "authorizedIdentities": [
  "@auth:role=enforcer"
  ],
  "description": "Created by $THIS"
}
EOF
}

_aws_auth_nodes()
{
cat <<EOF
{
  "authorizedNamespace": "$NAMESPACE",
  "allowsGet": false,
  "allowsPost": false,
  "allowsPut": false,
  "allowsDelete": false,
  "allowsPatch": false,
  "allowsHead": true,
  "propagate": true,
  "name": "Cluster Nodes",
  "subject": [
    [
      "@auth:account=$AWS_ACCOUNT",
      "@auth:realm=awssecuritytoken",
      "@auth:rolename=nodes.$FQDN"
    ]
  ],
  "authorizedIdentities": [
  "@auth:role=enforcer"
  ],
  "description": "Created by $THIS"
}
EOF
}

_cert()
{
cat <<EOF
apiVersion: certificates.k8s.io/v1beta1
kind: CertificateSigningRequest
metadata:
  name: enforcerd-client
spec:
  groups:
  - system:authenticated
  request: $(cat $CONFIG/key.csr | base64 | tr -d '\n')
  usages:
  - digital signature
  - key encipherment
  - client auth
EOF
}

